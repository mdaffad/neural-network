{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def linear(x, kwargs=None):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x, kwargs=None):\n",
    "    value = float(1 / (1 + math.exp(x * -1)))\n",
    "    threshold = kwargs.get(\"threshold\", None)\n",
    "    if threshold == None:\n",
    "        return value\n",
    "    else:\n",
    "        if value < threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "def relu(x, kwargs):\n",
    "    alpha = kwargs.get(\"alpha\", 0.0)\n",
    "    max_value = kwargs.get(\"max_value\", None)\n",
    "    threshold = 0\n",
    "    if x < threshold:\n",
    "        return max(x, x * alpha)\n",
    "    else:\n",
    "        if max_value == None:\n",
    "            return x\n",
    "        else:\n",
    "            return min(x, max_value)\n",
    "\n",
    "def softmax(arr, kwargs=None):\n",
    "    arr_exp = np.exp(arr)\n",
    "    return arr_exp / arr_exp.sum()\n",
    "\n",
    "def lossDerivative(targetj, oj):\n",
    "    return oj-targetj\n",
    "\n",
    "def lossFunction(targetj, oj, lenOutput=1):\n",
    "    loss = 0\n",
    "    if lenOutput>1:\n",
    "        for i in range(len(targetj)):\n",
    "            for j in range(len(targetj[i])):\n",
    "#                 print(targetj[i])\n",
    "                loss += (targetj[i][j]-oj[i][j]) ** 2\n",
    "    else:\n",
    "        loss += (targetj-oj) * (targetj-oj)\n",
    "    return loss/2\n",
    "\n",
    "def lossSoftmax(pk):\n",
    "    return -1*math.log(pk)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def softmaxDerivative(pj, targetClass=False):\n",
    "    if not targetClass:\n",
    "        return pj\n",
    "    else:\n",
    "        return -1*(1-pj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chainRuleOutputSigmoid (target, out_o) :\n",
    "    return -( target - out_o ) * out_o * ( 1 - out_o )\n",
    "def chainRuleOutputRelu (target, out_o):\n",
    "    return -( target - out_o ) * activation.activationFunction.reluDerivative(out_o)\n",
    "def chainRuleOutput2 (target, out_h, out_o, method) :\n",
    "    output = chainRuleOutputSigmoid(target, out_o)\n",
    "    return output * out_h\n",
    "\n",
    "def chainRuleHidden (arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, method) :\n",
    "    sum_Output = 0\n",
    "    if method == \"sigmoid\":\n",
    "        for j in range(len(arr_target)):\n",
    "            arr = []\n",
    "            output = chainRuleOutputSigmoid(arr_target[j], arr_out_o[j])\n",
    "            arr.append(output)\n",
    "            result = np.prod(arr) * arr_hiddenLayer_weight[j]\n",
    "            sum_Output += result\n",
    "        return sum_Output * out_h * ( 1 - out_h ) * vector_i\n",
    "    \n",
    "    elif method == \"relu\":\n",
    "        for j in range(len(arr_target)):\n",
    "            arr = []\n",
    "            output = 1\n",
    "            arr.append(output)\n",
    "            result = np.prod(arr) * arr_hiddenLayer_weight[j]\n",
    "            sum_Output += result\n",
    "        return sum_Output * out_h * ( 1 - out_h ) * vector_i\n",
    "    \n",
    "    elif method == \"relu\":\n",
    "        for j in range(len(arr_target)):\n",
    "            arr = []\n",
    "            output = chainRuleOutputRelu(arr_target[j], arr_out_o[j])\n",
    "            arr.append(output)\n",
    "            result = np.prod(arr) * arr_hiddenLayer_weight[j]\n",
    "            sum_Output += result\n",
    "        return sum_Output * out_h * ( 1 - out_h ) * vector_i\n",
    "    \n",
    "    elif method == \"softmax\":\n",
    "        for j in range(len(arr_target)):\n",
    "            arr = []\n",
    "            for k in range(len(arr_target[j])):\n",
    "                output = chainSoftMax(arr_target[j][k], arr_out_o[j][k], activation.activationFunction.softmax(arr_out_o[j]), out_h)\n",
    "                arr.append(output)\n",
    "            result = np.prod(arr) * arr_hiddenLayer_weight[j]\n",
    "            sum_Output += result\n",
    "        return sum_Output * vector_i\n",
    "\n",
    "def chainSoftMax (target, j, probJ, out_h) :\n",
    "    if (target == j):\n",
    "        return -( 1 - probJ ) * out_h\n",
    "    else:\n",
    "        return probJ * out_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def read_parameter():\n",
    "    # Opening JSON file \n",
    "    f = open('parameter.json',) \n",
    "    \n",
    "    # returns JSON object as  \n",
    "    # a dictionary \n",
    "    parameter = json.load(f) \n",
    "    # Closing file \n",
    "    f.close() \n",
    "    \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_model():\n",
    "    df = pd.read_csv('model.csv')\n",
    "    return df\n",
    "\n",
    "def read_data():\n",
    "    df = pd.read_csv('iris.csv')\n",
    "    df['species'] = df['species'].replace(['setosa'],1)\n",
    "    df['species'] = df['species'].replace(['versicolor'],2)\n",
    "    df['species'] = df['species'].replace(['virginica'],3)\n",
    "    # One hot encoding\n",
    "    y = pd.get_dummies(df.species, prefix='Class')\n",
    "#     print(y.head())\n",
    "    df[\"Class_1\"] = y[\"Class_1\"]\n",
    "    df[\"Class_2\"] = y[\"Class_2\"]\n",
    "    df[\"Class_3\"] = y[\"Class_3\"]\n",
    "#     print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation + Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Iris Dataset using Backpropagation + Feed Forward Neural Network\n",
      "\n",
      "   neuron activation\n",
      "0       3    sigmoid\n",
      "1       3    sigmoid\n",
      "2       3    sigmoid\n",
      "\n",
      "-Learning rate : 0.05\n",
      "-Error threshold : 0.05\n",
      "-Maximum iteration : 100\n",
      "-Batch Size : 2\n",
      "\n",
      "Accuracy \t:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "NEURON_INPUT = 4\n",
    "# For iterating per item in array. Except for linear function because single parameter is automate to iterate per item\n",
    "linear = np.vectorize(linear)\n",
    "sigmoid = np.vectorize(sigmoid)\n",
    "relu = np.vectorize(relu)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, base_layer, learning_rate=0.001, error_threshold=0.001, max_iter=100, batch_size=1):\n",
    "        self.base_layer = base_layer\n",
    "        self.current_layer = base_layer.copy()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.error_threshold = error_threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_total_layer(self):\n",
    "        return len(self.layer)\n",
    "\n",
    "    def enqueue_layer(self, layer):\n",
    "        self.current_layer.insert(0, layer)\n",
    "\n",
    "    def deque_layer(self):\n",
    "        self.current_layer.pop(0)\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        for idx in range(len(self.current_layer)):\n",
    "            if idx != 0:\n",
    "                self.current_layer[idx].input_value = self.current_layer[idx-1].result\n",
    "            self.current_layer[idx].compute()\n",
    "\n",
    "    def draw(self):\n",
    "        from graphviz import Digraph\n",
    "        f = Digraph('Feed Forward Neural Network', filename='ann1.gv')\n",
    "        f.attr('node', shape='circle', fixedsize='true', width='0.9')\n",
    "\n",
    "        for i in range(len(self.current_layer)):\n",
    "            if i != 0:\n",
    "                if i == 1:\n",
    "                    for j in range(len(self.current_layer[i].weight)):\n",
    "                        for k in range(len(self.current_layer[i].weight[j])):\n",
    "                            f.edge(f'x{j}', f'h{i}_{k}', str(\n",
    "                                self.current_layer[i].weight[j][k]))\n",
    "                    for j in range(len(self.current_layer[i].bias)):\n",
    "                        f.edge(f'bx', f'h{i}_{j}', str(\n",
    "                            self.current_layer[i].bias[j]))\n",
    "                else:\n",
    "                    for j in range(len(self.current_layer[i].weight)):\n",
    "                        for k in range(len(self.current_layer[i].weight[j])):\n",
    "                            f.edge(f'h{i-1}_{j}', f'h{i}_{k}',\n",
    "                                   str(self.current_layer[i].weight[j][k]))\n",
    "                    for j in range(len(self.current_layer[i].bias)):\n",
    "                        f.edge(f'bhx{i-1}', f'h{i}_{j}',\n",
    "                               str(self.current_layer[i].bias[j]))\n",
    "\n",
    "        f.view()\n",
    "\n",
    "    def learn(self, data):\n",
    "        # placeholder\n",
    "        current_iter = 0\n",
    "        target = []\n",
    "        result = []\n",
    "        for _ in range(self.max_iter):\n",
    "            error = 0.0\n",
    "            for index, item in data.iterrows():\n",
    "                # Prepare input\n",
    "                self.enqueue_layer(InputLayer(\n",
    "                    [item['sepal_length'], item['sepal_width'], item['petal_length'], item['petal_width']]))\n",
    "                # Forward andd result\n",
    "                self.forward_propagation()\n",
    "\n",
    "                target.append(\n",
    "                    [item['Class_1'], item['Class_2'], item['Class_3']])\n",
    "                result.append(self.current_layer[-1].result)\n",
    "                if self.current_layer[-1].activation_function_name == \"relu\" or self.current_layer[-1].activation_function_name == \"sigmoid\" or self.current_layer[-1].activation_function_name == \"linear\":\n",
    "                    error += lossFunction(target, result, 3)\n",
    "                elif self.current_layer[-1].activation_function_name == \"softmax\":\n",
    "                    for i in range(len(target)):\n",
    "                        for j in range(len(target[i])):\n",
    "                            if target[i][j] != result[i][j]:\n",
    "                                error += lossSoftmax(result[i][j])\n",
    "                if error < self.error_threshold:\n",
    "                    break\n",
    "\n",
    "                # cleaning layer\n",
    "                self.deque_layer()\n",
    "\n",
    "                # Learn with bach_size\n",
    "                if (index + 1) % self.batch_size == 0 or index == len(data.index):\n",
    "                    # backpropagation\n",
    "                    self.back_propagation(target, result)\n",
    "                    # clearing list and error foreach batch_size\n",
    "                    target.clear()\n",
    "                    result.clear()\n",
    "                    error = 0\n",
    "\n",
    "            if current_iter < self.max_iter:\n",
    "                break\n",
    "\n",
    "    def back_propagation(self, arr_target, arr_out):\n",
    "        for i in range(len(self.current_layer) - 1, -1, -1):\n",
    "            if i != len(self.current_layer) - 2 and i > 0:  # Not input or output layer\n",
    "                for j in range(len(self.current_layer[i].weight)):\n",
    "                    for k in range(len(self.current_layer[i].weight[j])):\n",
    "                        self.current_layer[i].weight[j][k] = self.current_layer[i].update_weight(arr_target, arr_out,\n",
    "                                                                                                 self.current_layer[i].weight[j], self.current_layer[i].result[j], self.current_layer[i].input_value[j], self.learning_rate)\n",
    "                for j in range(len(self.current_layer[i].bias)):\n",
    "                    self.current_layer[i].bias = self.current_layer[i].update_bias(arr_target, arr_out,\n",
    "                                                                                   self.current_layer[i].bias, self.current_layer[i].result[j], np.array([1 for x in range(len(self.current_layer[i].bias))]), self.learning_rate)\n",
    "            elif i == len(self.current_layer):\n",
    "                self.current_layer[i].weight = self.current_layer[i].update_weight_output(arr_target, arr_out,\n",
    "                                                                                          self.current_layer[i].weight, self.current_layer[i].result[j], self.current_layer[i].input_value[j], self.learning_rate)\n",
    "\n",
    "    def predict(self, data):\n",
    "        result = []\n",
    "        target = []\n",
    "        precise = 0\n",
    "        total_data = len(data.index)\n",
    "        for index, item in data.iterrows():\n",
    "            # Prepare input\n",
    "            self.enqueue_layer(InputLayer(\n",
    "                [item['sepal_length'], item['sepal_width'], item['petal_length'], item['petal_width']]))\n",
    "\n",
    "            # Forward andd result\n",
    "            self.forward_propagation()\n",
    "            target.append([item['Class_1'], item['Class_2'], item['Class_3']])\n",
    "            result.append(self.current_layer[-1].result)\n",
    "            max_index_col_result = np.argmax(result[-1], axis=0)\n",
    "            max_index_col_data = np.argmax(target[-1], axis=0)\n",
    "            if(max_index_col_data == max_index_col_result):\n",
    "                precise = precise + 1\n",
    "            self.deque_layer()\n",
    "        accuracy = 0.0\n",
    "        accuracy = float(precise / total_data)\n",
    "        print(\"Accuracy \\t: \", accuracy)\n",
    "\n",
    "\n",
    "class InputLayer:\n",
    "    def __init__(self, arr=[]):\n",
    "        self.input_value = np.array(arr)\n",
    "        self.result = self.input_value\n",
    "\n",
    "    def compute(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Layer(InputLayer):\n",
    "    def __init__(self, neuron_input, neuron_output, activation_function, activation_function_name, **kwargs):\n",
    "        super().__init__([])\n",
    "        self.weight = np.array([[1.5 * (1.0 - random.random())\n",
    "                                 for x in range(neuron_output)] for j in range(neuron_input)])\n",
    "        self.bias = np.array([1.5 * (1.0 - random.random())\n",
    "                              for x in range(neuron_output)])\n",
    "        self.result = np.array([])\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_name = activation_function_name\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def activate(self):\n",
    "        self.result = self.activation_function(self.result, self.kwargs)\n",
    "\n",
    "    def sigma(self):\n",
    "        # case 1 Dimension\n",
    "        if(len(self.weight[0]) == 1):\n",
    "            self.result = np.matmul(\n",
    "                self.input_value, self.weight.flatten()) + self.bias\n",
    "        else:\n",
    "            self.result = np.matmul(self.input_value, self.weight) + self.bias\n",
    "#         print(\"Sigma \\t: \", self.result)\n",
    "\n",
    "    def compute(self):\n",
    "#         print(\"Input \\t: \", self.input_value)\n",
    "        self.sigma()\n",
    "        self.activate()\n",
    "#         print(\"Weight \\t: \", self.weight)\n",
    "#         print(\"Result \\t: \", self.result)\n",
    "\n",
    "    def update_weight(self, arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, learning_rate):\n",
    "        return chainRuleHidden(arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, self.activation_function_name) * learning_rate * -1\n",
    "\n",
    "    def update_weight_output(self, arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, learning_rate):\n",
    "        return chainRuleOutput2(arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, self.activation_function_name) * learning_rate * -1\n",
    "\n",
    "    def update_bias(self, arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, learning_rate):\n",
    "        return chainRuleHidden(arr_target, arr_out_o, arr_hiddenLayer_weight, out_h, vector_i, self.activation_function_name) * learning_rate * -1\n",
    "\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, neuron, activation_function, **kwargs):\n",
    "        super().__init__(neuron, activation_function, **kwargs)\n",
    "        self.error([])\n",
    "\n",
    "def main():\n",
    "    parameter = read_parameter()\n",
    "    data = read_data()\n",
    "    model = read_model()\n",
    "    layer = []\n",
    "    learning_rate, error_threshold, max_iter, batch_size = \\\n",
    "        parameter[\"learning_rate\"], parameter[\"error_threshold\"], parameter[\"max_iter\"], parameter[\"batch_size\"]\n",
    "\n",
    "    # Create base layer\n",
    "#     print(\"Activation Layer: \")\n",
    "    for index, item in model.iterrows():\n",
    "        act = None\n",
    "        if (item['activation'] == 'sigmoid'):\n",
    "            act = sigmoid\n",
    "        elif (item['activation'] == 'linear'):\n",
    "            act = linear\n",
    "        elif (item['activation'] == 'relu'):\n",
    "            act = relu\n",
    "        elif (item['activation'] == 'softmax'):\n",
    "            act = softmax\n",
    "\n",
    "        # Case for near Input Layer or the Output Layer\n",
    "        if index == 0:\n",
    "            layer.append(\n",
    "                Layer(NEURON_INPUT, item['neuron'], act, item['activation'], threshold=0.1))\n",
    "        elif index > 0 and index != len(model.index):\n",
    "            layer.append(Layer(\n",
    "                model.iloc[index - 1, 0], item['neuron'], act, item['activation'], threshold=0.1))\n",
    "        elif index == model.index:\n",
    "            layer.append(OutputLayer(\n",
    "                model.iloc[index - 1, 0], item['neuron'], act, item['activation'], threshold=0.1))\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    # Build ANN model from layer and learn process\n",
    "    neural_network = NeuralNetwork(\n",
    "        layer, learning_rate, error_threshold, max_iter, batch_size)\n",
    "    neural_network.learn(data)\n",
    "\n",
    "    neural_network.predict(data)\n",
    "    neural_network.draw()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Learning Iris Dataset using Backpropagation + Feed Forward Neural Network\")\n",
    "    print(\"\")\n",
    "    print(read_model())\n",
    "    print(\"\")\n",
    "    print(\"-Learning rate : \" + str(read_parameter()[\"learning_rate\"]))\n",
    "    print(\"-Error threshold : \" + str(read_parameter()[\"error_threshold\"]))\n",
    "    print(\"-Maximum iteration : \" + str(read_parameter()[\"max_iter\"]))\n",
    "    print(\"-Batch Size : \" + str(read_parameter()[\"batch_size\"]))\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bit69d9728e61404ea2b004262d7f864172"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
